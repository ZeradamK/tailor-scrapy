# tailor-scrapy

The system is designed for reliability and scale: a FastAPI service accepts scrape jobs, a Redis queue decouples ingestion from crawling, and a Python worker uses Scrapy with Playwright (Chromium) to navigate JS‑heavy pages, perform on‑site searches, parse product pages, and stream items back in real time via Server‑Sent Events. Each job can carry its own proxy URL, headers, and cookies, enabling user‑IP or per‑user proxy affinity to blend in with normal traffic and reduce bot detection. I also implemented a rotating proxy pool with sticky sessions keyed per user.

Retailers are supported in two ways: dedicated spiders for core brands (e.g., H&M, Zara, UNIQLO, Nike) and a config‑driven generic spider powered by a retailers/config.json registry of domain selectors. This makes onboarding new stores fast add the domain’s search URL and CSS/XPath selectors to the config to start scraping item‑level data without code changes. To improve responsiveness and resilience, I added per‑domain throttling, retries, and a Redis cache keyed by (retailer, query) with TTL so repeat searches return in under a second. A simple /metrics endpoint exposes basic counters (jobs and items) for visibility. The worker exports JSON Lines to stdout and publishes each parsed product over Redis pub/sub immediately, which the Node backend consumes and relays to the UI for a real‑time search experience.

Search intent is optimized per retailer before hitting native site search, and the scraper extracts normalized product fields (title, price, currency, brand, image, product URL, availability, description—with rating/reviews where available). Everything is containerized: docker compose up -d brings up Redis, the API, and the worker; the Node backend points to the scraper via SCRAPER_BASE_URL; and the Next.js UI renders items as they arrive. The codebase is organized for extensibility and anti‑bot realism: Playwright headless browser control, user‑specific headers/cookies, optional residential proxies with sticky IP, and a fast path to add or harden retailer coverage either via config entries or dedicated spiders.
